{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TechNet 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "    \n",
    "nlp = spacy.load('en_core_web_sm',disable = ['ner','textcat'])\n",
    "with open('patents_ta_20191231_abstracts_fixed.pkl', 'rb') as f:\n",
    "    patents = pickle.load(f)\n",
    "with open('phrases_serhad_incl_nums.pkl','rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "def custom_tokenizer(nlp):\n",
    "    '''\n",
    "        Custom tokenizer for not to split words which include \"-\" and \"/\"\n",
    "    '''\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the functions below helps in extracting relevant information from the brackets and fixing the numbers, leading to proper splitting of sentences\n",
    "def text_remove(text):\n",
    "    \n",
    "    new_text = ''\n",
    "    ls = []\n",
    "    flag = 0\n",
    "    double_flag = False\n",
    "    brac_text = ''\n",
    "    for n in range(len(text)):\n",
    "        i = text[n]\n",
    "        if i == '(':\n",
    "            new_text = new_text[:-1]\n",
    "            flag += 1\n",
    "            if flag >1:\n",
    "                brac_text += i\n",
    "                double_flag = True\n",
    "        elif i == ')':\n",
    "            if flag > 1:\n",
    "                flag -=1\n",
    "                brac_text += i\n",
    "                continue\n",
    "            elif flag == 0:\n",
    "                continue\n",
    "                new_text += i\n",
    "            if brac_text.count(' ') == 0:\n",
    "                pass\n",
    "            else:\n",
    "                new_text += ' ' + brac_text + ' '\n",
    "            flag -= 1\n",
    "            brac_text = ''\n",
    "        elif flag == 0:\n",
    "            new_text += i\n",
    "        else:\n",
    "            brac_text += i\n",
    "    text = convert_num(new_text)\n",
    "    return text\n",
    "\n",
    "def convert_num(text):\n",
    "    flag1 = False\n",
    "    flag2 = 0\n",
    "    flag3 = False\n",
    "    flag4 = False\n",
    "    num = ''\n",
    "    new_text = ''\n",
    "    for i in text:\n",
    "        if i.isdigit():\n",
    "            num+=i\n",
    "            flag1 = True\n",
    "            flag3 = False\n",
    "            flag4 = False\n",
    "        elif i == '.' and flag1:\n",
    "            num+=i\n",
    "            flag2 +=1\n",
    "            flag3 = True\n",
    "        elif i == ',' and flag1:\n",
    "            flag4 = True\n",
    "        elif flag4:\n",
    "            flag1 = False\n",
    "            flag4 = False\n",
    "            if flag2 == 1:\n",
    "                num = float(num)\n",
    "                n=0\n",
    "                while num%1 != 0:\n",
    "                    num*=10\n",
    "                    n+=1\n",
    "                if n==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    new_text += str(int(num)) + 'x10^-' + str(n) + ' '\n",
    "            flag2 = 0\n",
    "            num = ''\n",
    "        elif flag1 and flag2 and (not flag3):\n",
    "            flag1 = False\n",
    "            if flag2 == 1:\n",
    "                num = float(num)\n",
    "                n=0\n",
    "                while num%1 != 0:\n",
    "                    num*=10\n",
    "                    n+=1\n",
    "                if n==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    new_text += str(int(num)) + 'x10^-' + str(n) + ' '\n",
    "            flag2 = 0\n",
    "            num = ''\n",
    "        elif flag1 and flag2 and flag3:\n",
    "            if flag2 == 1:\n",
    "                new_text += num\n",
    "            if flag2 == 2:\n",
    "                num = float(num[:-1])\n",
    "                n=0\n",
    "                while num%1 != 0:\n",
    "                    num*=10\n",
    "                    n+=1\n",
    "                if n==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    new_text += str(int(num)) + 'x10^-' + str(n) + '. '\n",
    "            flag2 = 0\n",
    "            num = ''\n",
    "        else:\n",
    "            flag1 = False\n",
    "            new_text += num + i\n",
    "            num = ''\n",
    "    if flag1 and flag2 and (not flag3):\n",
    "        if flag2 == 1:\n",
    "            num = float(num)\n",
    "            n=0\n",
    "            while num%1 != 0:\n",
    "                num*=10\n",
    "                n+=1\n",
    "            if n==0:\n",
    "                pass\n",
    "            else:\n",
    "                new_text += str(int(num)) + 'x10^-' + str(n) + ' '\n",
    "    elif flag1 and flag2 and flag3:\n",
    "        if flag2 == 1:\n",
    "            new_text += num\n",
    "        if flag2 == 2:\n",
    "            num = float(num[:-1])\n",
    "            n=0\n",
    "            while num%1 != 0:\n",
    "                num*=10\n",
    "                n+=1\n",
    "            if n==0:\n",
    "                pass\n",
    "            else:\n",
    "                new_text += str(int(num)) + 'x10^-' + str(n) + '. '\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions listed below use SpaCy and the clean text to find relations among terms by using verbs/qualitative relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos = {'NOUN':True,'VERB':True,'ADJ':True,'ADP':True,'PUNCT':True,'CCONJ':True,'DET':True,'PRON':True,'PROPN':True,'PART':True,'INJ':True,'SYM':True}\n",
    "fromby = {'from':True, 'by':True}\n",
    "dcfindVerb1 = {'xcomp':True,'advcl':True,'pcomp':True,'conj':True,'acl':True,'ccomp':True,'relcl':True}\n",
    "dcfindVerb2 = {'obj':True, 'pobj':True, 'dobj':True}\n",
    "dcfindVerb3 = {'NOUN':True, 'PROPN':True, 'PRON':True}\n",
    "dcfindVerb4 = {'subj':True, 'nsubj':True, 'nsubjpass':True, 'csubj':True, 'csubjpass':True,'obj':True, 'pobj':True, 'dobj':True}\n",
    "dcprep = {'subj':True, 'nsubj':True, 'nsubjpass':True, 'csubj':True, 'csubjpass':True,'obj':True, 'pobj':True, 'dobj':True,'ROOT':True}\n",
    "dcprep1 = {'to':True, 'in':True, 'with':True}\n",
    "dcprep2 = {'of':'has'}\n",
    "dcprep3 = {'NOUN':True,'PRON':True,'PROPN':True}\n",
    "dcprep4 = {'prep':True,'agent':True}\n",
    "dcxbyy = {'subj':True, 'nsubj':True, 'csubj':True}\n",
    "dcxbyy1 = {'nsubjpass':True, 'csubjpass':True}\n",
    "dcname = {'subj':True, 'nsubj':True, 'nsubjpass':True, 'csubj':True, 'csubjpass':True,'obj':True, 'pobj':True, 'dobj':True}\n",
    "dcextra = {'obj':True,'dobj':True,'pobj':True,'conj':True,'ROOT':True}\n",
    "\n",
    "\n",
    "def depSearch(dep, pos, parent, cand, current_level = 0, level_limit = -1):\n",
    "    \n",
    "#     parents = [(c, c.text, c.pos_, c.dep_, c.children) for c in parent.children]\n",
    "    parents = [(parent, parent.text,parent.pos_,parent.dep_,parent.children)]\n",
    "    init_cand =[x for x in cand]\n",
    "    current_level += 1\n",
    "    while level_limit==-1 or current_level<level_limit:\n",
    "        children = []\n",
    "        for par in parents:\n",
    "            item = [(c, c.text, c.pos_, c.dep_,  c.children) for c in par[4]]\n",
    "            if item != []:\n",
    "                children += item\n",
    "        if children == []:\n",
    "            return cand\n",
    "        else:\n",
    "            for child in children:\n",
    "                if dep.get(child[3],False) and pos.get(child[2],False):\n",
    "                    cand.append(child[0])\n",
    "        if init_cand != cand:\n",
    "            return cand\n",
    "        else:\n",
    "            parents = children\n",
    "        current_level += 1\n",
    "        \n",
    "def depSearch_rev(dep, pos, token, cand, current_level = 0, level_limit = -1):\n",
    "    \n",
    "    init_cand =[x for x in cand]\n",
    "    current_level += 1    \n",
    "    parent = token\n",
    "    while level_limit==-1 or current_level<level_limit:    \n",
    "        if parent.head == parent:\n",
    "            return current_level\n",
    "        else:\n",
    "            parent = parent.head\n",
    "            if dep.get(parent.dep_,False) and pos.get(parent.pos_,False):\n",
    "                cand.append(parent)\n",
    "        if init_cand != cand:\n",
    "            return current_level\n",
    "        current_level += 1\n",
    "        \n",
    "def depSearch1(dep, pos, parent, cand,  current_level = 0, level_limit = -1):\n",
    "\n",
    "    if level_limit==-1 or current_level<level_limit:\n",
    "        init_cand =[x for x in cand]\n",
    "        if [x for x in parent.children]!=[]:\n",
    "            children = [(c, c.text, c.pos_, c.dep_) for c in parent.children]\n",
    "        else:\n",
    "            return cand\n",
    "    else:\n",
    "        return cand\n",
    "    current_level += 1\n",
    "    if level_limit==-1 or current_level<level_limit:\n",
    "        for child in children:\n",
    "            if dep.get(child[3],False) and pos.get(child[2],False):\n",
    "                cand.append(child[0])\n",
    "        if cand!=init_cand:\n",
    "            return cand\n",
    "        else:\n",
    "            for child in children:\n",
    "                cand = depSearch1(dep, pos, child[0], cand, current_level, level_limit)\n",
    "            return cand\n",
    "    else:\n",
    "        return cand        \n",
    "\n",
    "def depSearch2(dep, pos, parent, cand, current_level = 0, level_limit = -1):\n",
    "    \n",
    "#     parents = [(c, c.text, c.pos_, c.dep_, c.children) for c in parent.children]\n",
    "    parents = [(parent, parent.text,parent.pos_,parent.dep_,parent.children)]\n",
    "    init_cand =[x for x in cand]\n",
    "    current_level += 1\n",
    "    while level_limit==-1 or current_level<level_limit:\n",
    "        children = []\n",
    "        for par in parents:\n",
    "            item = [(c, c.text, c.pos_, c.dep_,  c.children) for c in par[4]]\n",
    "            if item != []:\n",
    "                children += item\n",
    "        if children == []:\n",
    "            return cand\n",
    "        else:\n",
    "            flag = False\n",
    "            for child in children:\n",
    "                if child[2] == 'VERB':\n",
    "                    flag = True\n",
    "                elif dep.get(child[3],False) and pos.get(child[2],False):\n",
    "                    cand.append(child[0])\n",
    "            if flag:\n",
    "                return cand\n",
    "        if init_cand != cand:\n",
    "            return cand\n",
    "        else:\n",
    "            parents = children\n",
    "        current_level += 1\n",
    "\n",
    "def findInSubtree(dep, pos, parent, cand, current_level = 0, level_limit = -1):\n",
    "    \n",
    "    current_level += 1\n",
    "    if level_limit == -1 or current_level<=level_limit:\n",
    "        \n",
    "        for child in parent.children:\n",
    "            if pos.get(child.pos_,False) and dep.get(child.dep_,False):\n",
    "                cand.append(child)\n",
    "            findInSubtree(dep, pos, child, cand, current_level, level_limit)\n",
    "        current_level += -1\n",
    "        \n",
    "    return cand\n",
    "                                                        \n",
    "\n",
    "def findInSubtreeChain(dep, pos, parent, cand, current_level = 0, level_limit = -1):\n",
    "    current_level += 1\n",
    "    if level_limit == -1 or current_level<=level_limit:\n",
    "        cand_new = []\n",
    "        for child in parent.children:\n",
    "            if pos.get(child.pos_,False) and dep.get(child.dep_,False):\n",
    "                cand_new.append(child)\n",
    "                cand.append(child)\n",
    "        if cand_new:\n",
    "            for p in cand_new:\n",
    "                findInSubtreeChain(dep, pos, p, cand)\n",
    "                current_level -= 1\n",
    "\n",
    "    return cand\n",
    "dcfindROOT = {'obj':True, 'pobj':True, 'dobj':True}     \n",
    "def findROOT(tokens,triplets,RT_triplets): #ROOT is the enabling verb\n",
    "    for token in tokens:\n",
    "        if token.dep_ == 'ROOT': #ROOT is the enabling verb\n",
    "            n = token.i\n",
    "            subj = []\n",
    "            subjpass = []\n",
    "            obj = []\n",
    "            for i in token.children:\n",
    "                if dcxbyy.get(i.dep_,False):\n",
    "                    subj.append(i)\n",
    "                if dcxbyy1.get(i.dep_,False):\n",
    "                    subjpass.append(i)\n",
    "            depSearch(dcfindROOT, all_pos, token, obj, 0, -1)\n",
    "            if subj and obj:\n",
    "                for i in subj:\n",
    "                    for j in obj:\n",
    "                        try:\n",
    "                            if fromby.get(tokens[n+1].text,False) or fromby.get(tokens[n+2].text,False):\n",
    "                                triplets.append([j, token, i])\n",
    "                            else:\n",
    "                                triplets.append([i, token, j])\n",
    "                        except:\n",
    "                            triplets.append([i, token, j])\n",
    "            if subjpass and obj:\n",
    "                for i in subj:\n",
    "                    for j in obj:\n",
    "                        triplets.append([i, token, j])\n",
    "            if subjpass:\n",
    "                for i in subjpass:\n",
    "                    triplets.append(['Entity', token, i])\n",
    "                    for j in obj:\n",
    "                        RT_triplets.append([j, 'relatedTo', i])\n",
    "            if obj!=[]:\n",
    "                cand_list = []\n",
    "                for k in token.children:\n",
    "                    if dcprep4.get(k.dep_,False) and k.pos_ == 'ADP':\n",
    "                        depSearch(dcfindVerb2, dcfindVerb3, k, cand_list, 0, 2)\n",
    "                        parent = k\n",
    "                        flag = True\n",
    "                        while flag == True:\n",
    "                            flag = False\n",
    "                            for w in parent.children:\n",
    "                                if w.dep_ == 'conj' and w.pos_ == 'ADP':\n",
    "                                    depSearch(dcfindVerb2, dcfindVerb3, w, cand_list, 0, 2)\n",
    "                                    parent = w\n",
    "                                    flag = True\n",
    "                for k in obj:\n",
    "                    for y in cand_list:\n",
    "                        RT_triplets.append([k,'relatedTo',y])  \n",
    "            if subj!=[]:\n",
    "                for k in token.children:\n",
    "                    if k.dep_ == 'attr' and dcprep3.get(k.pos_,False):\n",
    "                        for y in subj:\n",
    "                            RT_triplets.append([y,'relatedTo',k])  \n",
    "            flag = True\n",
    "            parent = token\n",
    "            while flag:\n",
    "                flag = False\n",
    "                for z in parent.children:\n",
    "                    if z.dep_ == 'conj' and z.pos_ == 'VERB':\n",
    "                        flag = True\n",
    "                        parent = z\n",
    "                        n = z.i\n",
    "                        subj = []\n",
    "                        subjpass = []\n",
    "                        obj = []\n",
    "                        for i in z.children:\n",
    "                            if dcxbyy.get(i.dep_,False):\n",
    "                                subj.append(i)\n",
    "                            if dcxbyy1.get(i.dep_,False):\n",
    "                                subjpass.append(i)\n",
    "                        depSearch(dcfindROOT, all_pos, z, obj, 0, -1)\n",
    "                        if subj and obj:\n",
    "                            for i in subj:\n",
    "                                for j in obj:\n",
    "                                    try:\n",
    "                                        if fromby.get(tokens[n+1].text,False) or fromby.get(tokens[n+2].text,False):\n",
    "                                            triplets.append([j, z, i])\n",
    "                                        else:\n",
    "                                            triplets.append([i, z, j])\n",
    "                                    except:\n",
    "                                        triplets.append([i, z, j])\n",
    "                        if subjpass and obj:\n",
    "                            for i in subj:\n",
    "                                for j in obj:\n",
    "                                    triplets.append([i, z, j])\n",
    "                        if subjpass:\n",
    "                            for i in subjpass:\n",
    "                                triplets.append(['Entity', z, i])\n",
    "                                for j in obj:\n",
    "                                    RT_triplets.append([j, 'relatedTo', i])\n",
    "                        if obj!=[]:\n",
    "                            cand_list = []\n",
    "                            for k in z.children:\n",
    "                                if dcprep4.get(k.dep_) and k.pos_ == 'ADP':\n",
    "                                    depSearch(dcfindVerb2, dcfindVerb3, k, cand_list, 0, 2)\n",
    "                            for k in obj:\n",
    "                                for y in cand_list:\n",
    "                                    RT_triplets.append([k,'relatedTo',y])  \n",
    "                        if subj!=[]:\n",
    "                            for k in z.children:\n",
    "                                if k.dep_ == 'attr' and dcprep3.get(k.pos_,False):\n",
    "                                    for y in subj:\n",
    "                                        RT_triplets.append([y,'relatedTo',k])\n",
    "                        \n",
    "                        \n",
    "def findVerb(tokens,triplets,RT_triplets):\n",
    "    for token in tokens:\n",
    "        if dcfindVerb1.get(token.dep_) and token.pos_ == 'VERB':\n",
    "            tri = []\n",
    "            n = token.i\n",
    "            children = [x for x in token.children]\n",
    "            dir_obj_cands = []\n",
    "            dir_subj_cands = []\n",
    "            dir_subjpass_cands = []\n",
    "            for child in children:\n",
    "                if dcfindVerb3.get(child.pos_,False) and dcxbyy.get(child.dep_,False):\n",
    "                    dir_subj_cands.append(child)\n",
    "                elif dcfindVerb3.get(child.pos_,False) and dcxbyy1.get(child.dep_,False):\n",
    "                    dir_subjpass_cands.append(child)\n",
    "                elif dcfindVerb2.get(child.dep_,False):\n",
    "                    if dcfindVerb3.get(child.pos_,False):\n",
    "                        dir_obj_cands.append(child)\n",
    "                    else:\n",
    "                        cld = [x for x in child.children]\n",
    "                        for c in cld:\n",
    "                            if c.dep_ == 'conj' and c.pos_ == 'NOUN':\n",
    "                                dir_obj_cands.append(c)\n",
    "            if not dir_obj_cands:\n",
    "                indir_objs = []\n",
    "                findInSubtree(dcfindVerb2, dcfindVerb3, token, indir_objs, level_limit = 3)\n",
    "                if indir_objs:\n",
    "                    indir_objs = sorted([(x, np.abs(x.i-token.i)) for x in indir_objs], key = lambda x:x[1])\n",
    "                    indir_objs = [indir_objs[0][0]]\n",
    "            if dir_obj_cands and dir_subj_cands:\n",
    "                for subj in dir_subj_cands:\n",
    "                    for obj in dir_obj_cands:\n",
    "                        try:\n",
    "                            if fromby.get(tokens[n+1].text,False) or fromby.get(tokens[n+2].text,False):\n",
    "                                tri.append([obj, token, subj])\n",
    "                                triplets.append([obj, token, subj])\n",
    "                            else:\n",
    "                                tri.append([subj, token, obj])\n",
    "                                triplets.append([subj, token, obj])\n",
    "                        except:\n",
    "                            tri.append([subj, token, obj])\n",
    "                            triplets.append([subj, token, obj])\n",
    "            elif dir_obj_cands and dir_subjpass_cands:\n",
    "                for subj in dir_subjpass_cands:\n",
    "                    for obj in dir_obj_cands:\n",
    "                        tri.append([obj, token, subj])\n",
    "                        triplets.append([obj, token, subj])\n",
    "            elif (dir_subjpass_cands or dir_subj_cands) and indir_objs:\n",
    "                if dir_subjpass_cands:\n",
    "                    for subj in dir_subjpass_cands:\n",
    "                        for obj in indir_objs:\n",
    "                            tri.append([obj, token, subj])\n",
    "                            triplets.append([obj, token, subj])\n",
    "                elif dir_subj_cands:\n",
    "                    for subj in dir_subj_cands:\n",
    "                        for obj in indir_objs:\n",
    "                            try:\n",
    "                                if fromby.get(tokens[n+1].text,False)or fromby.get(tokens[n+2].text,False) :\n",
    "                                    tri.append([obj, token, subj])\n",
    "                                    triplets.append([obj, token, subj])\n",
    "                                else:\n",
    "                                    tri.append([subj, token, obj])\n",
    "                                    triplets.append([subj, token, obj])\n",
    "                            except:\n",
    "                                tri.append([subj, token, obj])\n",
    "                                triplets.append([subj, token, obj])\n",
    "            else:\n",
    "                head = token.head\n",
    "                count_ROOT = 0\n",
    "                pre_cands = []\n",
    "                count_rev = depSearch_rev(dcfindVerb4, dcfindVerb3, tokens[n], pre_cands)\n",
    "                post_cands = []\n",
    "                depSearch(dcfindVerb4, dcfindVerb3, tokens[n], post_cands)\n",
    "                while head.head != head:\n",
    "                    if head.dep_ == 'ROOT':\n",
    "                        break\n",
    "                    count_ROOT+=1\n",
    "                    head = head.head\n",
    "                if head.dep_ == 'ROOT' and count_ROOT<count_rev:\n",
    "                    n = token.i\n",
    "                    subj = []\n",
    "                    subjpass = []\n",
    "                    obj = []\n",
    "                    flag = True\n",
    "                    for i in triplets:\n",
    "                        if type(i[1]) != str:\n",
    "                            if i[1] == head:\n",
    "                                subj.append(i[0])\n",
    "                                flag = False\n",
    "                    if flag:\n",
    "                        for i in head.children:\n",
    "                            if dcxbyy.get(i.dep_,False):\n",
    "                                subj.append(i)\n",
    "                            elif dcxbyy1.get(i.dep_,False):\n",
    "                                subjpass.append(i)\n",
    "                    for i in token.children:\n",
    "                        if dcfindVerb2.get(i.dep_,False):\n",
    "                            obj.append(i)\n",
    "                    if obj == []:\n",
    "                        depSearch(dcfindVerb2, dcfindVerb3, tokens[n], obj)\n",
    "                    if subj and obj:\n",
    "                        for i in subj:\n",
    "                            for j in obj:\n",
    "                                tri.append([i, tokens[n], j])\n",
    "                                triplets.append([i, tokens[n], j])\n",
    "                    elif subjpass and obj:\n",
    "                        for i in subjpass:\n",
    "                            for j in obj:\n",
    "                                tri.append([i, tokens[n], j])\n",
    "                                triplets.append([i, tokens[n], j])\n",
    "                else:\n",
    "                    if pre_cands and post_cands:\n",
    "                        for i in range(len(post_cands)):\n",
    "                            for j in range(len(pre_cands)):\n",
    "                                try:\n",
    "                                    if fromby.get(tokens[n+1].text,False) or fromby.get(tokens[n+2].text,False):  \n",
    "                                        tri.append([post_cands[i], tokens[n], pre_cands[j]])\n",
    "                                        triplets.append([post_cands[i], tokens[n], pre_cands[j]])                                    \n",
    "                                    else:\n",
    "                                        tri.append([pre_cands[j], tokens[n], post_cands[i]])\n",
    "                                        triplets.append([pre_cands[j], tokens[n], post_cands[i]])\n",
    "                                except:\n",
    "                                    tri.append([pre_cands[j], tokens[n], post_cands[i]])\n",
    "                                    triplets.append([pre_cands[j], tokens[n], post_cands[i]])\n",
    "            if tri!=[]:\n",
    "                cand_list = []\n",
    "                for k in token.children:\n",
    "                    if dcprep4.get(k.dep_,False) and k.pos_ == 'ADP':\n",
    "                        depSearch(dcfindVerb2, dcfindVerb3, k, cand_list, 0, 2)\n",
    "                        parent = k\n",
    "                        flag = True\n",
    "                        while flag == True:\n",
    "                            flag = False\n",
    "                            for w in parent.children:\n",
    "                                if w.dep_ == 'conj' and w.pos_ == 'ADP':\n",
    "                                    depSearch(dcfindVerb2, dcfindVerb3, w, cand_list, 0, 2)\n",
    "                                    parent = w\n",
    "                                    flag = True\n",
    "                            \n",
    "#                     elif k.dep_ == 'attr' and dcprep3.get(k.pos_,False):\n",
    "#                         print(k)\n",
    "                for k in tri:\n",
    "                    for y in cand_list:\n",
    "                        RT_triplets.append([k[2],'relatedTo',y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPrep(tokens, triplets,RT_triplets):\n",
    "    \n",
    "    ntriplets = []\n",
    "    nRT_triplets = []\n",
    "    for n in range(len(tokens)):\n",
    "        ntri = []\n",
    "        nRT_tri = []\n",
    "        if dcprep4.get(tokens[n].dep_,False) and tokens[n].pos_ == 'ADP':\n",
    "            tre = []\n",
    "            if tokens[n].text == 'of':\n",
    "                tre = X_of_Y(tokens[n])\n",
    "            elif tokens[n].text == 'for':\n",
    "                tre = X_for_Y(tokens[n])\n",
    "            elif tokens[n].text == 'by':\n",
    "                tre = X_by_Y(tokens[n])\n",
    "            elif tokens[n].text == 'from':\n",
    "                tre = X_from_Y(tokens[n])\n",
    "            elif dcprep1.get(tokens[n].text,False) :\n",
    "                tre = X_with_Y(n, tokens)\n",
    "            if tre!=[]:\n",
    "                if type(tre[1]) == str:\n",
    "                    RT_triplets.append(tre)\n",
    "                else:\n",
    "                    triplets.append(tre)\n",
    "            else:\n",
    "                if dcprep3.get(tokens[n].head.pos_,False) and dcprep.get(tokens[n].dep_,False): \n",
    "                    cands = []\n",
    "                    depSearch(dcprep, dcprep3, tokens[n], cands)\n",
    "                    if tokens[n].head.pos_ == 'SCONJ':\n",
    "                        for k in tokens[n].head.children:\n",
    "                            first = k\n",
    "                    else:\n",
    "                        first = tokens[n].head\n",
    "                    if cands:\n",
    "                        if cands[0].pos_ == 'VERB':\n",
    "                            flag = 0\n",
    "                            for j in triplets:\n",
    "                                try:\n",
    "                                    if first in [j[0],j[2]] and cands[0] == j[1]:\n",
    "                                        flag = 1\n",
    "                                        break\n",
    "                                except:\n",
    "                                    pass\n",
    "                            if not flag:\n",
    "                                nRT_tri = [first,'relatedTo',cands[0]]\n",
    "                        else:\n",
    "                            flag = 0\n",
    "                            for j in triplets:\n",
    "                                try:\n",
    "                                    if first in [j[0],j[2]] and cands[0] in [j[0],j[2]]:\n",
    "                                        flag = 1\n",
    "                                        break\n",
    "                                except:\n",
    "                                    continue\n",
    "                            if not flag:\n",
    "                                ntri = [first,tokens[n],cands[0]]\n",
    "                if not (ntri+nRT_tri):\n",
    "                    if dcprep1.get(tokens[n].text,False):\n",
    "                        refer1 = n-1\n",
    "                        while refer1>=0 and dcprep3.get(tokens[refer1].pos_,False)==None or dcprep.get(tokens[refer1].dep_,False)==None:\n",
    "                            refer1-=1\n",
    "                        refer2 = n+1\n",
    "                        while refer2<len(tokens) and dcprep3.get(tokens[refer2].pos_,False)==None or dcprep.get(tokens[refer2].dep_,False)==None:\n",
    "                            refer2+=1\n",
    "                        if refer1>=0 and refer2<len(tokens):\n",
    "                            nRT_triplets.append([tokens[refer2],'relatedTo',tokens[refer1]])\n",
    "                if ntri:\n",
    "                    ntriplets.append(ntri)\n",
    "                elif nRT_tri:\n",
    "                    nRT_triplets.append(nRT_tri)\n",
    "    conjFinder(ntriplets,tokens)\n",
    "    conjFinder(ntriplets,tokens)\n",
    "    for i in ntriplets:\n",
    "        triplets.append(i)\n",
    "    for i in nRT_triplets:\n",
    "        RT_triplets.append(i)\n",
    "\n",
    "\n",
    "def X_of_Y (token):\n",
    "    \n",
    "    triplet = []\n",
    "    temp = []\n",
    "    if token.pos_ == 'ADP' and token.dep_ == 'prep':\n",
    "        children = [child for child in token.children]\n",
    "        for child in children:\n",
    "            if dcfindVerb2.get(child.dep_,False) and dcfindVerb3.get(child.pos_,False):\n",
    "                temp = [child, 'has']\n",
    "                break\n",
    "        if temp and token.head.pos_=='NOUN':\n",
    "            temp.append(token.head)\n",
    "            triplet = temp\n",
    "            return triplet\n",
    "    return triplet\n",
    "\n",
    "\n",
    "def X_for_Y (token):\n",
    "    \n",
    "    triplet = []\n",
    "    temp = []\n",
    "    if token.pos_ == 'ADP' and token.dep_ == 'prep':\n",
    "        temp.append(token.head)\n",
    "        children = [child for child in token.children]\n",
    "        flag = 0\n",
    "        for child in children:\n",
    "            if dcfindVerb2.get(child.dep_,False) and dcfindVerb3.get(child.pos_,False):\n",
    "                temp += ['relatedTo', child]\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            for child in children:\n",
    "                if child.dep_ == 'pcomp' and child.pos_ == 'VERB':\n",
    "                    obj_cands = []\n",
    "                    depSearch(dcfindVerb2, dcfindVerb3, child, obj_cands, level_limit = 3)\n",
    "                    if obj_cands:\n",
    "                        flag = 1\n",
    "                        temp += [child, obj_cands[0]]\n",
    "                        break\n",
    "        if flag:\n",
    "            triplet = temp\n",
    "    return triplet\n",
    "\n",
    "def X_by_Y (token):\n",
    "    \n",
    "    triplet = []\n",
    "    temp = []\n",
    "    flag = 0\n",
    "    if token.pos_ == 'ADP' and token.dep_ == 'prep':\n",
    "        temp.append(token.head)\n",
    "        children = [child for child in token.children]\n",
    "        for child in children:\n",
    "            if dcfindVerb2.get(child.dep_,False) and dcfindVerb3.get(child.pos_,False):\n",
    "                temp += ['relatedTo', child]\n",
    "                flag = 1\n",
    "                break\n",
    "    elif token.pos_ == 'ADP' and token.dep_ == 'agent' and token.head.pos_ == 'VERB':\n",
    "        \n",
    "        \n",
    "        subj_cands = []\n",
    "        depSearch(dcxbyy,dcfindVerb3, token.head, subj_cands)\n",
    "        subjpass_cands = []\n",
    "        depSearch(dcxbyy1, dcfindVerb3, token.head, subjpass_cands)\n",
    "        if subj_cands:\n",
    "            temp = [subj_cands[0], token.head]\n",
    "        elif subjpass_cands:\n",
    "            temp = [token.head, subjpass_cands[0]]\n",
    "        \n",
    "        else:\n",
    "            x = token.head\n",
    "            while x.pos_ != 'NOUN':\n",
    "                x = x.head\n",
    "            temp = [x, token.head]\n",
    "        children = [child for child in token.children]   \n",
    "        for child in children:\n",
    "            if dcfindVerb2.get(child.dep_,False) and dcfindVerb3.get(child.pos_,False):\n",
    "                flag = 1\n",
    "                if subjpass_cands:\n",
    "                    temp.insert(0, child)\n",
    "                else:\n",
    "                    temp.append(child)\n",
    "                break\n",
    "    if flag:\n",
    "        triplet = temp\n",
    "        triplet.reverse()\n",
    "    return triplet\n",
    "\n",
    "\n",
    "\n",
    "def X_from_Y (token):\n",
    "    \n",
    "    triplet = []\n",
    "    temp = []\n",
    "    flag = 0\n",
    "    if token.pos_ == 'ADP' and token.dep_ == 'prep' and token.head.pos_ == 'VERB':\n",
    "        objs_from = []\n",
    "        findInSubtree(dcfindVerb2, {'NOUN':True}, token, objs_from, current_level = 0, level_limit = 1)\n",
    "        objs_verb = []\n",
    "        findInSubtree(dcfindVerb2, {'NOUN':True}, token.head, objs_verb, current_level = 0, level_limit = 1)\n",
    "        if objs_from and objs_verb:\n",
    "            temp = [objs_verb[0], 'has', objs_from[0]]\n",
    "            flag = 1\n",
    "        \n",
    "    else:\n",
    "        temp.append(token.head)\n",
    "        children = [child for child in token.children]\n",
    "        for child in children:\n",
    "            if dcfindVerb2.get(child.dep_,False) and dcfindVerb3.get(child.pos_,False):\n",
    "                temp += ['relatedTo', child]\n",
    "                flag = 1\n",
    "                break\n",
    "    if flag:\n",
    "        triplet = temp\n",
    "    return triplet\n",
    "\n",
    "def X_with_Y (n, tokens):\n",
    "    \n",
    "    triplet = []\n",
    "#     high = min(n+3,len(tokens))\n",
    "#     low = max(n-3,0)\n",
    "    first = False\n",
    "    last = False\n",
    "    for i in range(n-1,-1):\n",
    "        if dcfindVerb3.get(tokens[i].pos_,False):\n",
    "            first = tokens[i]\n",
    "            break\n",
    "    for i in range(n+1,len(tokens)):\n",
    "        if dcfindVerb3.get(tokens[i].pos_,False):\n",
    "            last = tokens[i]\n",
    "            break\n",
    "    if first and last:\n",
    "        triplet = [first,'relatedTo',last]\n",
    "    return triplet\n",
    "\n",
    "\n",
    "def X_PART_Y (tokens,triplets):\n",
    "    \n",
    "    temp = []\n",
    "    for token in tokens:\n",
    "        if token.pos_ == 'PART':\n",
    "            triplets.append([token.head, 'has', token.head.head])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameCheck(triplets):\n",
    "    \n",
    "    dc = {}\n",
    "    items = ['set','number','groups','who','that','therewith'] #and more!\n",
    "    for i in items:\n",
    "        dc[i] = True\n",
    "    n = 0\n",
    "    while n < len(triplets):\n",
    "        i = triplets[n]\n",
    "        try:\n",
    "            if i[0] == i[2]:\n",
    "                triplets.pop(n)\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        flag = False\n",
    "        if type(i[0]) != str:\n",
    "            if dc.get(i[0].text.lower(),False):\n",
    "                change = False\n",
    "                for j in i[0].children:\n",
    "                    if j.dep_ == 'prep':\n",
    "                        for k in j.children:\n",
    "                            if dcfindVerb4.get(k.dep_,False):\n",
    "                                change = k\n",
    "                                break\n",
    "                    elif dcfindVerb4.get(j.dep_,False):\n",
    "                        change = j\n",
    "                        break\n",
    "                if change:\n",
    "                    try:\n",
    "                        if i[2] != change:\n",
    "                            i[0] = change\n",
    "                            change = False \n",
    "                    except:\n",
    "                        i[0] = change\n",
    "                        change = False\n",
    "#                 check the head or child, you need to check for amod\n",
    "        if type(i[2]) != str:\n",
    "            if dc.get(i[2].text.lower(),False):\n",
    "                change = False\n",
    "                for j in i[2].children:\n",
    "                    if j.dep_ == 'prep':\n",
    "                        for k in j.children:\n",
    "                            if dcfindVerb4.get(k.dep_,False):\n",
    "                                change = k\n",
    "                                break\n",
    "                    elif dcfindVerb4.get(j.dep_,False):\n",
    "                        change = j\n",
    "                        break\n",
    "                if change:\n",
    "                    try:\n",
    "                        if i[0] != change:\n",
    "                            i[2] = change\n",
    "                            change = False \n",
    "                    except:\n",
    "                        i[2] = change\n",
    "                        change = False \n",
    "        n+=1\n",
    "    \n",
    "    \n",
    "def conjFinder (triplets, doc):\n",
    "    \n",
    "    new_triplets = []\n",
    "    for triplet in triplets:\n",
    "        conj0 = []\n",
    "        conj1 = []\n",
    "        conj2 = []\n",
    "        if type(triplet[0]) != str:\n",
    "            depSearch1({'conj':True}, dcfindVerb3, triplet[0], conj0, 0, 2)        \n",
    "            if triplet[0].dep_ == 'conj' and triplet[0].head.pos_ == 'NOUN':\n",
    "                conj0.append(triplet[0].head)\n",
    "        conj2 = []\n",
    "        if type(triplet[2]) != str:\n",
    "            depSearch1({'conj':True}, dcfindVerb3, triplet[2], conj2, 0, 2)        \n",
    "            if triplet[2].dep_ == 'conj' and triplet[2].head.pos_ == 'NOUN':\n",
    "                conj2.append(triplet[2].head)\n",
    "        if conj0:\n",
    "            new_triplets += [[x]+triplet[1:] for x in conj0]\n",
    "        if conj2:\n",
    "            new_triplets += [triplet[:2]+[x] for x in conj2]\n",
    "        if type(triplet[1]) != str:\n",
    "            depSearch1({'conj':True}, {'VERB':True}, triplet[1], conj1, 0, 2)        \n",
    "            for i in conj1:\n",
    "                obj = []\n",
    "                depSearch({'conj':True}, {'VERB':True}, i, obj, 0, 2)\n",
    "                for j in obj:\n",
    "                    triplets.append([triplet[0],i,j])\n",
    "    for i in new_triplets:\n",
    "        flag = True\n",
    "        for j in triplets:\n",
    "            try:\n",
    "                if i==j:\n",
    "                    flag = False\n",
    "            except:\n",
    "                continue\n",
    "        if flag:\n",
    "            triplets.append(i)\n",
    "        \n",
    "def findNeg(tokens,triplets):\n",
    "    \n",
    "    for token in tokens:\n",
    "        if 'neg' == token.dep_:\n",
    "            for i in triplets:\n",
    "                if type(i[1])!=str:\n",
    "                    if i[1] == token.head:\n",
    "                        #triplets\n",
    "                        i.append('!')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq(triplets,RT_triplets):\n",
    "    \n",
    "    dc = {}\n",
    "    for i in triplets:\n",
    "        if len(i) == 3:\n",
    "            dc[i[0]+'~'+i[1]+'~'+i[2]] = True\n",
    "        else:\n",
    "            dc[i[0]+'~!'+i[1]+'~'+i[2]] = True\n",
    "    n = []\n",
    "    for i in list(dc.keys()):\n",
    "        n.append([j for j in i.split('~')])\n",
    "            \n",
    "    dc_RT = {}\n",
    "    dc_ext = {}\n",
    "    for i in RT_triplets:\n",
    "        if i[1] == 'relatedTo':\n",
    "            if i[0]<i[2]:\n",
    "                dc_RT[i[0]+'~relatedTo~'+i[2]] = True\n",
    "            else:\n",
    "                dc_RT[i[2]+'~relatedTo~'+i[0]] = True\n",
    "        else:\n",
    "            dc_ext[i[0]+'~'+i[1]+'~'+i[2]] = True\n",
    "            \n",
    "    for i in list(dc_ext.keys()):\n",
    "        n.append([j for j in i.split('~')])\n",
    "        \n",
    "    dc = {}\n",
    "    for i in n:\n",
    "        if i[0]<i[2]:\n",
    "            dc[i[0]+'~'+i[2]] = True\n",
    "        else:\n",
    "            dc[i[2]+'~'+i[0]] = True\n",
    "        \n",
    "    for i in list(dc_RT.keys()):\n",
    "        a = [j for j in i.split('~')]\n",
    "        if dc.get(a[0]+'~'+a[2],False):\n",
    "            pass\n",
    "        else:\n",
    "            n.append(a)\n",
    "    return n\n",
    "                \n",
    "def findExtra(tokens, triplets):\n",
    "    for token in tokens:\n",
    "        if dcextra.get(token.dep_) and dcfindVerb3.get(token.pos_):\n",
    "            obj = []\n",
    "            depSearch2(dcextra,dcfindVerb3,token,obj)\n",
    "            for i in obj:\n",
    "                triplets.append([token,'relatedTo',i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(triplets,RT_triplets,sent,vocab):\n",
    "    new_tri = []\n",
    "    trips = []\n",
    "    RT_trips = []\n",
    "    for n in range(len(triplets)):\n",
    "        i = triplets[n]\n",
    "        try:\n",
    "            if not dcfindVerb3.get(i[0].pos_,False):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if not dcfindVerb3.get(i[2].pos_,False):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i[0] = phraseCompleter(i[0],sent,vocab,new_tri)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i[1] = lemmatizer.lemmatize(i[1].text, 'v')\n",
    "        except:\n",
    "            i[1] = lemmatizer.lemmatize(i[1], 'v')\n",
    "        try:\n",
    "            i[2] = phraseCompleter(i[2],sent,vocab,new_tri)\n",
    "        except:\n",
    "            pass  \n",
    "        trips.append(i)\n",
    "    for n in range(len(RT_triplets)):\n",
    "        i = RT_triplets[n]\n",
    "        try:\n",
    "            if not dcfindVerb3.get(i[0].pos_,False):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if not dcfindVerb3.get(i[2].pos_,False):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i[0] = phraseCompleter(i[0],sent,vocab,new_tri)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i[2] = phraseCompleter(i[2],sent,vocab,new_tri)\n",
    "        except:\n",
    "            pass\n",
    "        RT_trips.append(i)\n",
    "    return new_tri + trips, RT_trips\n",
    "    \n",
    "\n",
    "def phraseCompleter(token, tokens, vocab, new_tri):\n",
    "\n",
    "#     tokens = [x for x in doc]\n",
    "    ind_token = token.i       \n",
    "    i = ind_token-1\n",
    "    temp = []\n",
    "    while ind_token-4<i and i>-1:\n",
    "        word = tokens[i]\n",
    "        if (word.dep_ == 'compound' or word.dep_[-3:] == 'mod') and word.head.i<=ind_token:\n",
    "            temp.append(word)\n",
    "            i-=1\n",
    "        else:\n",
    "            break\n",
    "    temp.reverse()\n",
    "#     temp = [i for i in tokens[max(ind_token-3, 0):ind_token] if (i.dep_ == 'compound' or i.dep_[-3:] == 'mod')]\n",
    "    attr = []\n",
    "    flag = False\n",
    "    for i in range(len(temp)):\n",
    "        tempstr = ' '.join([x.text.lower() for x in temp[i:]])\n",
    "        if token.pos_ == 'NOUN':\n",
    "            tempstr = ' '.join([tempstr]+[lemmatizer.lemmatize(token.text, 'n')])\n",
    "        else:\n",
    "            tempstr = ' '.join([tempstr]+[token.text])\n",
    "        tempstr = tempstr.lower()\n",
    "        if vocab.get(tempstr,False):\n",
    "            flag = True \n",
    "            break\n",
    "    if flag:\n",
    "        return tempstr\n",
    "    else:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            return token.lemma_.lower()\n",
    "        else:\n",
    "            return token.text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few basic examples to check if the functions work on short yet confusing (for the computer) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Lysozymes, destroy, bacteria], [Lysozymes, protect, animals]]\n"
     ]
    }
   ],
   "source": [
    "sent = nlp('Lysozymes destroy bacteria to protect animals.')\n",
    "triplets = []\n",
    "findROOT(sent,triplets,triplets)\n",
    "findVerb(sent,triplets,triplets)\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Entity', destroyed, Bacteria], [animals, 'relatedTo', Bacteria], ['Entity', protect, animals]]\n"
     ]
    }
   ],
   "source": [
    "sent = nlp('Bacteria are destroyed to protect animals.')\n",
    "triplets = []\n",
    "findROOT(sent,triplets,triplets)\n",
    "findVerb(sent,triplets,triplets)\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Lysozymes, destroy, bacteria], [Lysozymes, protecting, animals]]\n"
     ]
    }
   ],
   "source": [
    "sent = nlp('Lysozymes destroy bacteria, protecting animals.')\n",
    "triplets = []\n",
    "findROOT(sent,triplets,triplets)\n",
    "findVerb(sent,triplets,triplets)\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[lysozyme, protects, animals], [lysozyme, destroying, bacteria]]\n"
     ]
    }
   ],
   "source": [
    "sent = nlp('By destroying invading bacteria, lysozyme protects animals.')\n",
    "triplets = []\n",
    "findROOT(sent,triplets,triplets)\n",
    "findVerb(sent,triplets,triplets)\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[lysozymes, destroy, bacteria], [lysozymes, protect, animals]]\n"
     ]
    }
   ],
   "source": [
    "sent = nlp('To protect animals, lysozymes destroy bacteria.')\n",
    "triplets = []\n",
    "findROOT(sent,triplets,triplets)\n",
    "findVerb(sent,triplets,triplets)\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Lysozymes, destroy, bacteria], [Lysozymes, protect, animals]]\n"
     ]
    }
   ],
   "source": [
    "sent = nlp('Lysozymes destroy bacteria and protect animals')\n",
    "triplets = []\n",
    "findROOT(sent,triplets,triplets)\n",
    "findVerb(sent,triplets,triplets)\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[order, refine, gold], [order, thermosetting, gold], [order, evaporating, impurities]]\n"
     ]
    }
   ],
   "source": [
    "sent = nlp(\"In order to refine gold, thermosetting purifies gold by evaporating the impurities. the the\")\n",
    "triplets = []\n",
    "findROOT(sent,triplets,triplets)\n",
    "findVerb(sent,triplets,triplets)\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patent Dataset to check the performance of the functions created aboce and will be used later to create the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10402966\n",
      "Systems and methods for identifying optimized ablation targets for treating and preventing arrhythmias sustained by reentrant circuits. Methods and systems for identifying optimized ablation targets for treating and preventing arrhythmias sustained by reentrant circuits are described. The methods comprise receiving at least one mesh generated from one or more images of a patient's heart, receiving activation data generated from one or more simulations of electrical-signal propagation over the at least one mesh, generating at least one flow graph based on the activation data and the at least one mesh, and applying a max-flow min-cut algorithm to the at least one flow graph to determine at least one of a number, one or more dimensions, and one or more locations of one or more ablation targets. Non-transitory computer-readable media storing a set of instructions for treating and preventing arrhythmias sustained by reentrant circuits are also described.\n",
      "['ablation target', 'treat', 'arrhythmia']\n",
      "['ablation target', 'prevent', 'arrhythmia']\n",
      "['reentrant circuit', 'sustain', 'arrhythmia']\n",
      "['system', 'identify', 'ablation target']\n",
      "['method', 'identify', 'ablation target']\n",
      "['Entity', 'describe', 'method']\n",
      "['Entity', 'describe', 'system']\n",
      "['method', 'comprise', 'mesh']\n",
      "['method', 'comprise', 'activation data']\n",
      "['method', 'receive', 'mesh']\n",
      "['image', 'generate', 'mesh']\n",
      "['method', 'receive', 'activation data']\n",
      "['simulation', 'generate', 'activation data']\n",
      "['mesh', 'generate', 'activation data']\n",
      "['method', 'generate', 'flow graph']\n",
      "['flow graph', 'base', 'activation data']\n",
      "['method', 'apply', 'min-cut algorithm']\n",
      "['flow graph', 'base', 'mesh']\n",
      "['method', 'determine', 'dimension']\n",
      "['method', 'determine', 'location']\n",
      "['patient', 'have', 'heart']\n",
      "['Entity', 'describe', 'non-transitory computer-readable medium']\n",
      "['non-transitory computer-readable medium', 'store', 'instruction']\n",
      "['instruction', 'treat', 'arrhythmia']\n",
      "['instruction', 'prevent', 'arrhythmia']\n",
      "['heart', 'has', 'image']\n",
      "['propagation', 'has', 'simulation']\n",
      "['ablation target', 'has', 'location']\n",
      "['instruction', 'has', 'set']\n",
      "['method', 'relatedTo', 'system']\n",
      "['flow graph', 'relatedTo', 'min-cut algorithm']\n",
      "['dimension', 'relatedTo', 'location']\n",
      "3960278\n",
      "Lamp cap assembly. The present invention relates to electric light sources such as may be used in sealed beam lighting units. An electric lamp assembly comprises a sealed envelope containing a light source mounted in a flanged lamp cap. Terminal pins pass through hermetic seals in the cap and support the light source in a predetermined position relative to the cap flange. \n",
      "['invention', 'relate', 'electric light source']\n",
      "['invention', 'use', 'sealed beam lighting unit']\n",
      "['electric lamp assembly', 'comprise', 'sealed envelope']\n",
      "['sealed envelope', 'contain', 'light source']\n",
      "['light source', 'mount', 'lamp cap']\n",
      "['terminal pin', 'pass', 'hermetic seal']\n",
      "['terminal pin', 'pass', 'light source']\n",
      "['terminal pin', 'support', 'light source']\n",
      "['hermetic seal', 'relatedTo', 'light source']\n",
      "['light source', 'relatedTo', 'position']\n",
      "['cap', 'relatedTo', 'hermetic seal']\n",
      "['cap flange', 'relatedTo', 'position']\n",
      "4060573\n",
      "Carburetor assembly. A carburetor assembly for stratified charge combustion type engines wherein a rich set carburetor and a lean set carburetor are arranged such that a substantially straight line connects together the centers of an inlet bore of the rich set carburetor and inlet bores of a primary side carburetor and a secondary side carburetor of the lean set carburetor as seen from above. The rich set carburetor is disposed adjacent the primary side carburetor of the lean set carburetor and formed integrally therewith. A float chamber is arranged such that the center of its inclination is disposed in the vicinity of the center line between the main fuel nozzles of the primary side carburetor and the secondary side carburetor. An open end of the main fuel nozzle of the rich set carburetor is disposed at a level higher than open ends of the main fuel nozzles of the primary side carburetor and the secondary side carburetor of the lean set carburetor. \n",
      "['Entity', 'arrange', 'carburetor assembly']\n",
      "['straight line', 'connect', 'center']\n",
      "['Entity', 'arrange', 'float chamber']\n",
      "['inclination', 'dispose', 'center']\n",
      "['Entity', 'dispose', 'open end']\n",
      "['inlet bore', 'has', 'center']\n",
      "['rich set carburetor', 'has', 'inlet bore']\n",
      "['primary side carburetor', 'has', 'inlet bore']\n",
      "['lean set carburetor', 'has', 'secondary side carburetor']\n",
      "['lean set carburetor', 'has', 'primary side carburetor']\n",
      "['inclination', 'has', 'center']\n",
      "['center line', 'has', 'vicinity']\n",
      "['primary side carburetor', 'has', 'main fuel nozzle']\n",
      "['main fuel nozzle', 'has', 'open end']\n",
      "['rich set carburetor', 'has', 'main fuel nozzle']\n",
      "['carburetor assembly', 'relatedTo', 'rich set carburetor']\n",
      "['carburetor assembly', 'relatedTo', 'center']\n",
      "['carburetor assembly', 'relatedTo', 'type engine']\n",
      "['lean set carburetor', 'relatedTo', 'rich set carburetor']\n",
      "['primary side carburetor', 'relatedTo', 'secondary side carburetor']\n",
      "['primary side carburetor', 'relatedTo', 'rich set carburetor']\n",
      "['float chamber', 'relatedTo', 'vicinity']\n",
      "['center', 'relatedTo', 'vicinity']\n",
      "['center line', 'relatedTo', 'main fuel nozzle']\n",
      "4143024\n",
      "Thermoplastic polycarbonate moulding compositions with improved ease of mould release. The invention relates to a thermoplastic moulding composition comprising a high molecular, thermoplastic aromatic polycarbonate based on an aromatic dihydroxy compound and containing 001 to 5x10^-1 by weight of an ester of a saturated aliphatic carboxylic acid with 10 to 20 C. atoms per molecule and an aromatic hydroxy compound with from 1 to 6 OH groups. \n",
      "['invention', 'relate', 'thermoplastic moulding composition']\n",
      "['thermoplastic moulding composition', 'comprise', 'thermoplastic aromatic polycarbonate']\n",
      "['thermoplastic aromatic polycarbonate', 'base', 'aromatic dihydroxy compound']\n",
      "['thermoplastic aromatic polycarbonate', 'contain', 'weight']\n",
      "['ester', 'has', 'weight']\n",
      "['saturated aliphatic carboxylic acid', 'has', 'ester']\n",
      "['improved ease', 'relatedTo', 'polycarbonate moulding composition']\n",
      "['c', 'relatedTo', 'ester']\n",
      "['aromatic hydroxy compound', 'relatedTo', 'atom']\n",
      "['aromatic hydroxy compound', 'relatedTo', 'oh group']\n",
      "5310498\n",
      "Sulfur dioxide removal from gases using a modified lime. A wet scrubbing process for removing sulfur dioxide from combustion gases uses an aqueous slurry containing calcium components resulting from the slaking of lime, with the slaked lime formed by mixing lime with water containing a calcium sulfur-oxide salt. The presence of the calcium sulfur oxide salt in the slaking water results in a more easily dewatered sludge that is subsequently removed from the wet scrubbing system. \n",
      "['wet scrubbing process', 'use', 'aqueous slurry']\n",
      "['wet scrubbing process', 'remove', 'sulfur dioxide']\n",
      "['aqueous slurry', 'contain', 'calcium component']\n",
      "['slaking', 'result', 'calcium component']\n",
      "['lime', 'form', 'slaked lime']\n",
      "['slaked lime', 'mix', 'lime']\n",
      "['water', 'contain', 'calcium sulfur-oxide salt']\n",
      "['wet scrubbing system', 'remove', 'dewatered sludge']\n",
      "['sulfur dioxide', 'has', 'combustion gas']\n",
      "['lime', 'has', 'slaking']\n",
      "['calcium sulfur oxide salt', 'has', 'presence']\n",
      "['gas', 'relatedTo', 'lime']\n",
      "['gas', 'relatedTo', 'sulfur dioxide removal']\n",
      "['aqueous slurry', 'relatedTo', 'slaked lime']\n",
      "['lime', 'relatedTo', 'water']\n",
      "['calcium sulfur oxide salt', 'relatedTo', 'dewatered sludge']\n",
      "['dewatered sludge', 'relatedTo', 'presence']\n",
      "['calcium sulfur oxide salt', 'relatedTo', 'slaking water result']\n",
      "5728779\n",
      "Powder paint of epoxy-reactive polymer and aliphatic chain-containing polyepoxide. A binder composition for thermosetting powder paints comprising a polymer that is capable of reacting with epoxy groups and a crosslinker that contains epoxy groups, wherein the crosslinker comprises at least one C5 to C26 linear or branched aliphatic chain with the proviso that epoxy groups are carried on at least one aliphatic chain. In order to function as a crosslinker the amount of oxirane-oxygen originating from the crosslinker in the binder composition is higher than 1x10^-1 meq/gram. The crosslinker is, for example, an epoxydized oil, a modified epoxydized oil or an epoxydized alkyd resin. A powder paint comprising the binder composition can contain a suitable catalyst and, optionally, an additional curing agent. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['powder paint', 'comprise', 'polymer']\n",
      "['polymer', 'react', 'epoxy group']\n",
      "['crosslinker', 'comprise', 'c5']\n",
      "['aliphatic chain', 'carry', 'epoxy group']\n",
      "['binder composition', 'thermosetting', 'powder paint']\n",
      "['polymer', 'react', 'crosslinker']\n",
      "['crosslinker', 'contain', 'epoxy group']\n",
      "['amount', 'be', 'order']\n",
      "['order', 'function', 'crosslinker']\n",
      "['crosslinker', 'be', 'example']\n",
      "['paint', 'contain', 'catalyst']\n",
      "['paint', 'comprise', 'binder composition']\n",
      "['epoxy-reactive polymer', 'has', 'powder paint']\n",
      "['oxirane-oxygen', 'has', 'amount']\n",
      "['polyepoxide', 'relatedTo', 'powder paint']\n",
      "['c5', 'relatedTo', 'proviso']\n",
      "['c26', 'relatedTo', 'c5']\n",
      "['c5', 'relatedTo', 'linear']\n",
      "['branched aliphatic chain', 'relatedTo', 'linear']\n",
      "['crosslinker', 'relatedTo', 'originating']\n",
      "['binder composition', 'relatedTo', 'crosslinker']\n",
      "['crosslinker', 'relatedTo', 'oil']\n",
      "['alkyd resin', 'relatedTo', 'oil']\n",
      "6279073\n",
      "Configurable synchronizer for double data rate synchronous dynamic random access memory. A configurable synchronizer for DDR-SDRAM is provided that includes a strobe select module operable to receive a memory select signal and to pass strobe signals 30 from one or more DDR-SDRAMs 18 to a number of synchronizer circuits corresponding to data signals passed in parallel by each DDR-SDRAM as indicated by the memory select signal. A rising edge latch receives a rising edge data signal and latches the rising edge data signal through the rising edge latch on a rising edge of the strobe signal. A falling edge latch receives a falling edge data signal and latches the falling edge data signal through the falling edge latch on a falling edge of the strobe signal. A data signal selector receives a data order control signal and forwards the rising edge data signal from the rising edge latch to an intermediate output on either a rising edge of a memory clock cycle or a falling edge of a memory clock cycle followed by forwarding the falling edge data signal from the falling edge latch to the intermediate output on an opposite edge of the memory clock cycle in response to the data order control signal. An output latch receives the intermediate output and latches the intermediate output through the output latch to an output signal on each core clock cycle. \n",
      "['Entity', 'provide', 'synchronizer']\n",
      "['synchronizer', 'include', 'select module']\n",
      "['select module', 'receive', 'memory select signal']\n",
      "['select module', 'pass', 'strobe signal']\n",
      "['synchronizer circuit', 'correspond', 'data signal']\n",
      "['data signal', 'pass', 'parallel']\n",
      "['data signal', 'pass', 'ddr-sdram']\n",
      "['memory select signal', 'indicate', 'data signal']\n",
      "['ddr-sdram', 'pass', 'data signal']\n",
      "['rising edge latch', 'receive', 'rising edge data signal']\n",
      "['rising edge latch', 'latch', 'rising edge data signal']\n",
      "['falling edge latch', 'receive', 'falling edge data signal']\n",
      "['falling edge latch', 'latch', 'falling edge data signal']\n",
      "['signal selector', 'receive', 'control signal']\n",
      "['falling edge data signal', 'follow', 'memory clock cycle']\n",
      "['memory clock cycle', 'forward', 'falling edge data signal']\n",
      "['output latch', 'receive', 'intermediate output']\n",
      "['output latch', 'latch', 'intermediate output']\n",
      "['strobe signal', 'has', 'ddr-sdrams']\n",
      "['synchronizer circuit', 'has', 'number']\n",
      "['strobe signal', 'has', 'rising edge']\n",
      "['strobe signal', 'has', 'falling edge']\n",
      "['memory clock cycle', 'has', 'rising edge']\n",
      "['memory clock cycle', 'has', 'falling edge']\n",
      "['falling edge data signal', 'has', 'falling edge latch']\n",
      "['memory clock cycle', 'has', 'opposite edge']\n",
      "['dynamic random access memory', 'relatedTo', 'synchronizer']\n",
      "['ddr-sdram', 'relatedTo', 'synchronizer']\n",
      "['ddr-sdram', 'relatedTo', 'parallel']\n",
      "['rising edge', 'relatedTo', 'rising edge data signal']\n",
      "['falling edge', 'relatedTo', 'falling edge data signal']\n",
      "['falling edge data signal', 'relatedTo', 'intermediate output']\n",
      "['falling edge data signal', 'relatedTo', 'opposite edge']\n",
      "['falling edge data signal', 'relatedTo', 'response']\n",
      "['memory clock cycle', 'relatedTo', 'response']\n",
      "['intermediate output', 'relatedTo', 'rising edge']\n",
      "['control signal', 'relatedTo', 'response']\n",
      "['intermediate output', 'relatedTo', 'output signal']\n",
      "['core clock cycle', 'relatedTo', 'output signal']\n",
      "7296552\n",
      "Air intake structure for engine. An air intake system for an engine of an outboard motor includes an air intake support member that provides support for other components of the air intake system. The other component of the air intake system that can be supported by the air intake support member include a throttle valve assembly, plenum chambers, and air intake passages. The air intake passages can be made longer to increase engine performance and can be manufactured with less material because the air intake passages do not need to provide support for the air intake system.\n",
      "['air intake system', 'include', 'support member']\n",
      "['support member', 'provide', 'support']\n",
      "['component', 'include', 'throttle valve assembly']\n",
      "['support member', 'support', 'component']\n",
      "['component', 'include', 'plenum chamber']\n",
      "['component', 'include', 'air intake passage']\n",
      "['Entity', 'make', 'air intake passage']\n",
      "['Entity', 'increase', 'engine performance']\n",
      "['Entity', 'manufacture', 'material']\n",
      "['air intake passage', '!need', 'support']\n",
      "['Entity', 'provide', 'support']\n",
      "['outboard motor', 'has', 'engine']\n",
      "['air intake system', 'has', 'component']\n",
      "['air intake structure', 'relatedTo', 'engine']\n",
      "['air intake system', 'relatedTo', 'engine']\n",
      "['component', 'relatedTo', 'support']\n",
      "['plenum chamber', 'relatedTo', 'throttle valve assembly']\n",
      "['air intake passage', 'relatedTo', 'plenum chamber']\n",
      "['air intake passage', 'relatedTo', 'engine performance']\n",
      "['air intake system', 'relatedTo', 'support']\n",
      "8660118\n",
      "Methods, systems, and computer readable media for next hop scaling. The subject matter described herein includes a packet forwarding device that implements next hop scaling. Rather than storing a complete set of next hop bindings at each packet processor, the storage of next hop bindings is distributed among packet processors in the packet forwarding device such that each packet processor stores next hop bindings for the hosts that are directly connected to the packet processor. For hosts that are not directly connected to a packet processor, the packet processor stores relay entries. Because of the distributed storage of next hop bindings, the number of hosts that can be served by a single packet forwarding device is increased over packet forwarding devices where each packet processor stores a complete set of next hop bindings for all connected hosts.\n",
      "['method', 'readable', 'medium']\n",
      "['system', 'readable', 'medium']\n",
      "['subject matter', 'describe', 'packet forwarding device']\n",
      "['herein', 'include', 'packet forwarding device']\n",
      "['packet forwarding device', 'implement', 'next hop scaling']\n",
      "['Entity', 'distribute', 'storage']\n",
      "['Entity', 'store', 'packet processor']\n",
      "['host', 'connect', 'packet processor']\n",
      "['processor store', 'relay', 'entry']\n",
      "['processor store', '!connect', 'host']\n",
      "['processor store', '!connect', 'packet processor']\n",
      "['Entity', 'increase', 'host']\n",
      "['single packet forwarding device', 'serve', 'host']\n",
      "['next hop binding', 'has', 'packet processor']\n",
      "['next hop binding', 'has', 'storage']\n",
      "['next hop binding', 'has', 'distributed storage']\n",
      "['host', 'has', 'number']\n",
      "['next hop binding', 'has', 'connected host']\n",
      "['medium', 'relatedTo', 'next hop']\n",
      "['medium', 'relatedTo', 'scaling']\n",
      "['packet processor', 'relatedTo', 'storage']\n",
      "['host', 'relatedTo', 'next hop binding']\n",
      "['complete set', 'relatedTo', 'next hop binding']\n",
      "['complete set', 'relatedTo', 'packet processor']\n",
      "['packet forwarding device', 'relatedTo', 'packet processor']\n",
      "['entry', 'relatedTo', 'relay']\n",
      "['distributed storage', 'relatedTo', 'host']\n",
      "['host', 'relatedTo', 'packet forwarding device']\n",
      "['distributed storage', 'relatedTo', 'packet forwarding device']\n",
      "['complete set', 'relatedTo', 'connected host']\n",
      "['complete set', 'relatedTo', 'packet forwarding device']\n",
      "9437756\n",
      "Metallization of solar cells using metal foils. A solar cell structure includes P-type and N-type doped regions. A dielectric spacer is formed on a surface of the solar cell structure. A metal layer is formed on the dielectric spacer and on the surface of the solar cell structure that is exposed by the dielectric spacer. A metal foil is placed on the metal layer. A laser beam is used to weld the metal foil to the metal layer. A laser beam is also used to pattern the metal foil. The laser beam ablates portions of the metal foil and the metal layer that are over the dielectric spacer. The laser ablation of the metal foil cuts the metal foil into separate P-type and N-type metal fingers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solar cell structure', 'include', 'region']\n",
      "['p-type', 'dope', 'region']\n",
      "['n-type', 'dope', 'region']\n",
      "['Entity', 'form', 'dielectric spacer']\n",
      "['Entity', 'form', 'metal layer']\n",
      "['dielectric spacer', 'expose', 'solar cell structure']\n",
      "['Entity', 'place', 'metal foil']\n",
      "['Entity', 'use', 'laser beam']\n",
      "['Entity', 'weld', 'metal foil']\n",
      "['Entity', 'pattern', 'metal foil']\n",
      "['laser beam', 'ablate', 'portion']\n",
      "['laser ablation', 'cut', 'metal foil']\n",
      "['solar cell', 'has', 'metallization']\n",
      "['solar cell structure', 'has', 'surface']\n",
      "['metal foil', 'has', 'portion']\n",
      "['metal foil', 'has', 'laser ablation']\n",
      "['metal foil', 'relatedTo', 'solar cell']\n",
      "['dielectric spacer', 'relatedTo', 'surface']\n",
      "['dielectric spacer', 'relatedTo', 'metal layer']\n",
      "['metal foil', 'relatedTo', 'metal layer']\n",
      "['laser beam', 'relatedTo', 'metal foil']\n",
      "['metal foil', 'relatedTo', 'separate p-type']\n",
      "['n-type metal finger', 'relatedTo', 'separate p-type']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pat_no = ['10402966','3960278','4060573','4143024','5310498','5728779','6279073','7296552','8660118','9437756']\n",
    "tre4 = []\n",
    "for i in pat_no:\n",
    "    triplets = []\n",
    "    RT_triplets = []\n",
    "    texts = []\n",
    "    print(i)\n",
    "    if patents[i]['title']:\n",
    "        title = patents[i]['title']\n",
    "    else:\n",
    "        title = \"\"\n",
    "    if patents[i]['abstract']:\n",
    "        abstract = patents[i]['abstract']\n",
    "    else:\n",
    "        abstract = \"\"\n",
    "    text = title + '. ' + abstract\n",
    "    if title or abstract:\n",
    "        text = text_remove(text)\n",
    "        text = text.replace('.sub.', '')\n",
    "        text = text.replace('  ',' ')\n",
    "        text = text.replace('.sup.','')\n",
    "        text = text.replace('.dergree.','')\n",
    "        text = text.replace('`',\"'\")\n",
    "        text = text.replace('~','')\n",
    "        text = text.replace('FIG. ','figure ')\n",
    "        text = text.replace('e.g.','example')\n",
    "        text = text.replace('i.e.','that is')\n",
    "        for l in text.split('. '):\n",
    "            for j in l.split('; '):\n",
    "                k = j.replace('.','')\n",
    "                k = k.replace(';','')\n",
    "                texts.append(j)\n",
    "    print(text)\n",
    "    for j in texts:\n",
    "        sent = nlp(j)\n",
    "        findROOT(sent,triplets,RT_triplets)\n",
    "        findVerb(sent,triplets,RT_triplets)\n",
    "        findPrep(sent,triplets,RT_triplets)\n",
    "#         for i in triplets + RT_triplets:\n",
    "#             print(i)\n",
    "        nameCheck(triplets)\n",
    "        nameCheck(RT_triplets)\n",
    "        conjFinder(triplets,sent)\n",
    "        conjFinder(triplets,sent)\n",
    "        conjFinder(triplets,sent)\n",
    "        findNeg(sent,triplets)\n",
    "        X_PART_Y(sent,triplets)\n",
    "        findExtra(sent,RT_triplets)\n",
    "        triplets,RT_triplets = refine(triplets,RT_triplets,sent,vocab)\n",
    "    triplets = uniq(triplets,RT_triplets)\n",
    "    for j in triplets:\n",
    "        tre4.append(j)\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create relations for the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-7f1671055e68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.sub.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-0f2424d29ede>\u001b[0m in \u001b[0;36mtext_remove\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mbrac_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mflag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mnew_text\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mbrac_text\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this code block helps in getting clean sentences with the help of text_remove, convert_num, removing noise and splitting the sentences for us\n",
    "texts = []\n",
    "for a in list(patents.keys()):\n",
    "    if patents[a]['title']:\n",
    "        title = patents[a]['title']\n",
    "    else:\n",
    "        title = \"\"\n",
    "    if patents[a]['abstract']:\n",
    "        abstract = patents[a]['abstract']\n",
    "    else:\n",
    "        abstract = \"\"\n",
    "    text = title + '. ' + abstract\n",
    "    if title or abstract:\n",
    "        text = text_remove(text)\n",
    "        text = text.replace('.sub.', '')\n",
    "        text = text.replace('  ',' ')\n",
    "        text = text.replace('.sup.','')\n",
    "        text = text.replace('.dergree.','')\n",
    "        text = text.replace('`',\"'\")\n",
    "        text = text.replace('~','')\n",
    "        text = text.replace('FIG. ','figure ')\n",
    "        text = text.replace('e.g.','example')\n",
    "        text = text.replace('i.e.','that is')\n",
    "        for i in text.split('. '):\n",
    "            for j in i.split('; '):\n",
    "                k = j.replace('.','')\n",
    "                k = k.replace(';','')\n",
    "                texts.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "findROOT() missing 1 required positional argument: 'RT_triplets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-27b26bf018f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mfindROOT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfindVerb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfindPrep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: findROOT() missing 1 required positional argument: 'RT_triplets'"
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "for sent in nlp.pipe(texts[:1000], n_threads=20, batch_size=100):\n",
    "    tri = []\n",
    "    findROOT(sent,tri)\n",
    "    findVerb(sent,tri)\n",
    "    findPrep(sent,tri)\n",
    "    nameCheck(tri)\n",
    "    conjFinder(tri,sent)\n",
    "    conjFinder(tri,sent)\n",
    "    conjFinder(tri,sent)\n",
    "    findNeg(sent,tri)\n",
    "    X_PART_Y(sent,tri)\n",
    "    findExtra(sent,tri)\n",
    "    refine(tri,sent,vocab)\n",
    "    for i in tri:\n",
    "        triplets.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
